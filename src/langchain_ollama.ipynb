{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install rdflib\n",
    "!pip install load_dotenv\n",
    "!pip install faiss-cpu\n",
    "!pip install --upgrade langchain-ollama\n",
    "!pip install --upgrade langchain \n",
    "!pip install --upgrade langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import json\n",
    "import requests\n",
    "from PIL import Image\n",
    "from pprint import pprint\n",
    "from dotenv import load_dotenv\n",
    "from rdflib import Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain.chat_models import ChatOllama\n",
    "from langchain_core.messages import AIMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "from uuid import uuid4\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration files\n",
    "ttl_file = \"/Users/adrian/Desktop/items_filtered.ttl\"\n",
    "pkl_file = \"/Users/adrian/Desktop/dataset.pkl\"\n",
    "txt_for_validation = \"/Users/adrian/Desktop/graph_output.txt\"\n",
    "docs_path = \"/Users/adrian/Desktop/momu\"\n",
    "vec_path = \"/Users/adrian/Desktop/vector_store\"\n",
    "context_path = \"/Users/adrian/Desktop/context.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_ttl_to_dict(ttl_file, pkl_file, txt_for_validation):\n",
    "    out = dict()\n",
    "    g = Graph()\n",
    "    g.parse(ttl_file, format=\"turtle\")\n",
    "    json_ld_data = g.serialize(format=\"json-ld\", indent=4)\n",
    "    with open(txt_for_validation, \"w\") as file:\n",
    "        # Iterate through each triple in the graph\n",
    "        subjects = set(g.subjects())\n",
    "        for subject in subjects:\n",
    "            org_subject = subject\n",
    "            if \"api/item\" in subject:\n",
    "                subj_formalized = str(subject).split('/')[-1]\n",
    "                if '#' in subj_formalized:\n",
    "                    subject = subj_formalized.split('#')[-1]\n",
    "                else:\n",
    "                    subject = subj_formalized\n",
    "                out[subject] = {}\n",
    "                file.write(f\"Subject {subject}\\n\")\n",
    "                # Iterate over all triples where this subject is the subject\n",
    "                for pred, obj in g.predicate_objects(subject=org_subject):\n",
    "                    pred_formalized = str(pred).split('/')[-1]\n",
    "                    if '#' in pred_formalized:\n",
    "                        pred = pred_formalized.split('#')[-1]\n",
    "                    else:\n",
    "                        pred = pred_formalized\n",
    "                    # if \"http\" not in obj:\n",
    "                    if \"http\" not in obj or pred == \"P48_has_preferred_identifier\":\n",
    "\n",
    "                        out[subject][pred] = str(obj)\n",
    "                        file.write(f\"{pred}: {obj}\\n\")\n",
    "                     \n",
    "    with open(pkl_file, \"wb\") as f:\n",
    "        pickle.dump(out, f)\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = convert_ttl_to_dict(ttl_file, pkl_file, txt_for_validation)\n",
    "# with open(pkl_file, \"rb\") as f:\n",
    "#     dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify_predicate(predicate):\n",
    "    match = re.search(r\"[#/](\\w+)$\", predicate)\n",
    "    return match.group(1).replace(\"_\", \" \") if match else predicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_readable_content_v2(instance, properties):\n",
    "    lines = []\n",
    "    instance_id = str(instance).split(\"/\")[-1]  \n",
    "    # lines.append(f\"The item {instance_id} has the following information:\")\n",
    "    \n",
    "    for predicate, obj in properties:\n",
    "        simplified_predicate = simplify_predicate(str(predicate))\n",
    "        if \"is_public\" in str(predicate):\n",
    "            lines.append(f\"The item {instance_id} is {'public' if obj == 'true' else 'not public'}.\")\n",
    "        elif \"title\" in str(predicate):\n",
    "            lines.append(f\"The identifier of this artifact is \\\"{obj}\\\".\")\n",
    "        elif \"description\" in str(predicate):\n",
    "            lines.append(f\"The description of this artifact is \\\"{obj}\\\"\")\n",
    "        elif \"date\" == str(predicate):\n",
    "            # print (predicate, obj)\n",
    "            lines.append(f\"This artifact was created from the following period: {obj}.\")\n",
    "        elif \"modified\" in str(predicate):\n",
    "            lines.append(f\"This artifact was last modified on {obj}.\")\n",
    "        elif \"medium\" in str(predicate):\n",
    "            lines.append(f\"The medium of this artifact includes {obj}.\")\n",
    "        elif \"extent\" in str(predicate):\n",
    "            lines.append(f\"The dimensions of this artifact are {obj}.\")\n",
    "        elif \"publisher\" in str(predicate):\n",
    "            lines.append(f\"The publisher of this artifact is {obj}.\")\n",
    "        elif \"subject\" in str(predicate):\n",
    "            lines.append(f\"The subject of this artifact includes {obj}.\")\n",
    "        elif \"shortDescription\" in str(predicate):\n",
    "            obj = obj.replace('\\n', '')\n",
    "            lines.append(f\"The context of this artifact is \\\"{obj}\\\".\")\n",
    "        elif \"P48_has_preferred_identifier\" in str(predicate):\n",
    "            lines.append(f\"The preferred identifier of this artifact is {obj}.\")\n",
    "        elif \"P50_has_current_keeper\" in str(predicate):\n",
    "            lines.append(f\"The current keeper of this artifact is {obj}.\")\n",
    "        elif \"P55_has_current_location\" in str(predicate):\n",
    "            lines.append(f\"The current location of this artifact is in {obj}.\")\n",
    "        elif \"dateSubmitted\" in str(predicate):\n",
    "            lines.append(f\"This artifact was submitted on {obj}.\")\n",
    "        elif \"identifierGroupType\" in str(predicate):\n",
    "            lines.append(f\"The group type of this artifact is  {obj}.\")\n",
    "        elif \"identifierGroupValue\" in str(predicate):\n",
    "            lines.append(f\"The group value of this artifact is {obj}.\")\n",
    "        elif simplified_predicate == \"id\":\n",
    "            continue\n",
    "        else:     \n",
    "            # print (simplified_predicate)\n",
    "            lines.append(f\"The {simplified_predicate} of this artifact is {obj}.\")\n",
    "    \n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pkl_to_doc(dataset, docs_path, context_path, save_txt = True):\n",
    "\n",
    "    docs = list()\n",
    "    combined_texts = dict()\n",
    "    for item, val in dataset.items():\n",
    "        item_id = str(item).split(\"/\")[-1]\n",
    "        properties = [(k, v) for k,v in val.items()]\n",
    "        # lines = generate_readable_content(item, properties)\n",
    "        lines = generate_readable_content_v2(item, properties)\n",
    "        for line in lines:\n",
    "            docs.append (\n",
    "                Document(\n",
    "                page_content = line,\n",
    "                metadata={'item_id': item_id}\n",
    "                )\n",
    "        )\n",
    "\n",
    "        combined_texts[item_id] = '\\n'.join(lines)\n",
    "        \n",
    "        if save_txt:\n",
    "            file_name = item_id + \".txt\"  \n",
    "            file_path = os.path.join(docs_path, file_name)\n",
    "            with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write('\\n'.join(lines))\n",
    "                \n",
    "    with open(context_path, 'wb') as f:\n",
    "        pickle.dump(combined_texts, f)\n",
    "        \n",
    "    return docs, combined_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs, combined_texts = convert_pkl_to_doc(dataset, docs_path, context_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_retriever(item_id):\n",
    "    \n",
    "    BASE = \"https://heron.libis.be/momu/api/items/\"\n",
    "    full_url = BASE + str(item_id)\n",
    "    response = requests.get(full_url)\n",
    "    data_dict = json.loads(response.content)\n",
    "    # image = Image.open(io.BytesIO(data_dict[\"thumbnail_display_urls\"][\"large\"]))\n",
    "    image = data_dict[\"thumbnail_display_urls\"][\"large\"]\n",
    "    return image\n",
    "\n",
    "# text and image\n",
    "def create_context(combined_texts):\n",
    "    \n",
    "    context_dict = dict()\n",
    "    for item_id, full_text in combined_texts.items():\n",
    "        context_dict[item_id] = dict()\n",
    "        context_dict[item_id]['full_text'] = full_text\n",
    "        context_dict[item_id]['image'] = image_retriever(item_id)\n",
    "    \n",
    "    with open(context_path, 'wb') as f:\n",
    "        pickle.dump(context_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_context(combined_texts)\n",
    "with open(context_path, 'rb') as f:\n",
    "    context_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://python.langchain.com/docs/integrations/vectorstores/faiss/\n",
    "\n",
    "# embeddings = OllamaEmbeddings(model=\"llama3.2:1b\")\n",
    "embeddings = OllamaEmbeddings(model=\"mxbai-embed-large\")\n",
    "\n",
    "def txt_to_vec(docs, embeddings, vec_path):\n",
    "    # embeddings = OllamaEmbeddings(model=\"mxbai-embed-large\")\n",
    "\n",
    "    index = faiss.IndexFlatL2(len(embeddings.embed_query(\"hello world\")))\n",
    "\n",
    "    vector_store = FAISS(\n",
    "        embedding_function=embeddings,\n",
    "        index=index,\n",
    "        docstore=InMemoryDocstore(),\n",
    "        index_to_docstore_id={},\n",
    "    )\n",
    "\n",
    "    uuids = [str(uuid4()) for _ in range(len(docs))]\n",
    "    vector_store.add_documents(documents=docs, ids=uuids)\n",
    "    vector_store.save_local(vec_path)\n",
    "    \n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = txt_to_vec(docs, embeddings, vec_path)\n",
    "vector_store = FAISS.load_local(vec_path, embeddings, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* This artifact was submitted on 2022-06-28. [{'item_id': '16148'}]\n",
      "* This artifact was submitted on 2022-06-28. [{'item_id': '15966'}]\n",
      "* This artifact was submitted on 2022-06-28. [{'item_id': '16157'}]\n",
      "* This artifact was submitted on 2022-06-28. [{'item_id': '16099'}]\n",
      "* This artifact was submitted on 2022-06-28. [{'item_id': '15812'}]\n"
     ]
    }
   ],
   "source": [
    "# TEST DEMO: Similarity search\n",
    "results = vector_store.similarity_search(\n",
    "    \"which artifact was submitted on 06-28\",\n",
    "    k=5,\n",
    ")\n",
    "for res in results:\n",
    "    print(f\"* {res.page_content} [{res.metadata}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_llm(llm, query, context_dict):\n",
    "    results = vector_store.similarity_search(\n",
    "        query,\n",
    "        k=1,\n",
    "        # filter={\"source\": \"tweet\"},\n",
    "    )\n",
    "    # for res in results:\n",
    "    #     print(f\"* {res.page_content} [{res.metadata}]\")\n",
    "    \n",
    "    # retrieve the right artifact \n",
    "    # simply pick the top one\n",
    "    item_id = results[0].metadata['item_id']\n",
    "    context = context_dict[item_id][\"full_text\"]\n",
    "    \n",
    "    messages = [\n",
    "        (\n",
    "            \"system\",\n",
    "            f\"You are a helpful assistant in museum to explain the artifact. \\\n",
    "            You have the knowledge about the artifact: {context}. \\\n",
    "            Please answer the question \\\n",
    "            and then introduce detailed information about this artifact, \\\n",
    "            Your answer must include the identifier, created period, and 3-4 sentences as its description \",\n",
    "        ),\n",
    "        (\"human\", query),\n",
    "    ]\n",
    "    \n",
    "    ai_msg = llm.invoke(messages)\n",
    "    print('-'* 30 + \" Context of the Artifact \" + '-'* 30)\n",
    "    print(context)\n",
    "    print('-'* 30 + \" LLM answer \" + '-'* 30)\n",
    "    print(ai_msg.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://python.langchain.com/docs/integrations/chat/ollama/\n",
    "llm = ChatOllama(model=\"llama3.1\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ Context of the Artifact ------------------------------\n",
      "The identifier of this artifact is \"ST2014\".\n",
      "This artifact was created from the following period: 1930-1959.\n",
      "This artifact was submitted on 2022-06-28.\n",
      "The description of this artifact is \"Jurk, daagse jurk of werkjurk.\"\n",
      "The publisher of this artifact is MOMU.\n",
      "The subject of this artifact includes jurk.\n",
      "The context of this artifact is \"Deze daagse huis- of werkjurk van bedrukt katoen is nog ongedragen. Het papieren etiket geeft inzicht in waar dit kledingstuk gekocht is: de Grand Magasins À Saint-Jacques te Reims.  Aan het begin van de jaren 1920 fuseren twee grote warenhuizen in Reims, La Samaritaine en À la Tour Saint-Jacques, onder de naam À Saint-Jacques. In deze nieuwe zaak in de Rue de Vesle verkoopt men voornamelijk dames- en kinderkleding. Tevens zijn er zijn verstelateliers, en klanten kunnen er terecht voor stoffen, interieurtextiel en een grote verscheidenheid aan accessoires. In een bijgebouw wordt heren-, jongens-, en werkkleding verkocht. De winkel wordt uiteindelijk in 1971 overgenomen door de Britse keten Marks & Spencer.  Ruimvallende jurken zoals dit exemplaar werden door vrouwen binnenshuis over de eigen kleding gedragen om deze te beschermen tijdens het (huishoudelijk) werk. De taille wordt bijeengehouden met een ceintuur, en de jurk heeft praktische ruime zakken. Door de knopen langs de gehele voorkant is de jurk snel uit te trekken indien er bezoek komt en men er deftig uit wil zien. Het is onzeker waarom deze jurk nooit gedragen is. Mogelijk komt de jurk uit een onverkochte winkelvoorraad van À Saint-Jacques.\".\n",
      "The preferred identifier of this artifact is http://data.momu.be/ark:34546/mjm68z.\n",
      "The current keeper of this artifact is Studiecollectie.\n",
      "The identifier of this artifact is ST2014.\n",
      "The group type of this artifact is  TMS.\n",
      "The group value of this artifact is 60190.\n",
      "The partNumber of this artifact is 74348.\n",
      "------------------------------ LLM answer ------------------------------\n",
      "The artifact that was created from the period of 1930-1959 is ST2014.\n",
      "\n",
      "Here's a detailed introduction to this artifact:\n",
      "\n",
      "**Identifier:** ST2014\n",
      "**Created Period:** 1930-1959\n",
      "**Description:** This artifact, a daily house or work dress made of printed cotton, has never been worn. The paper label provides insight into where it was purchased: the Grand Magasins À Saint-Jacques in Reims. The dress is designed to be worn over one's own clothing for protection while doing household chores. It features practical large pockets and can be quickly removed by unfastening the buttons along the front, making it suitable for social visits.\n"
     ]
    }
   ],
   "source": [
    "query = \"which artifact was created from the following period: 1930-1959?\"\n",
    "chat_llm(llm, query, context_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# 1) improve the retrieval stage (acc & multi questions)\n",
    "# 2) work on the system prompt to see what to display\n",
    "# 3) backup the vector store \n",
    "# 4) reduce the inference time\n",
    "# 5) introduce more metadata, e.g., image\n",
    "# 6) multilingual feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_llm_multilingual(llm, query, item_id, context_dict):\n",
    "\n",
    "    context = context_dict[item_id]['full_text']\n",
    "    \n",
    "    messages = [\n",
    "        (\n",
    "            \"system\",\n",
    "            f\"You are a helpful assistant in museum to explain the artifact in multiple languages. \\\n",
    "            You have the knowledge about this artifact: {context}. \\\n",
    "            Please first detect the language of the question (you don't need to explictly output the detected language), \\\n",
    "            and then answer the question in the detected language, \\\n",
    "            Your answer must include the identifier\",\n",
    "        ),\n",
    "        (\"human\", query),\n",
    "    ]\n",
    "    \n",
    "    ai_msg = llm.invoke(messages)\n",
    "    print('-'* 30 + \" Context of the Artifact \" + '-'* 30)\n",
    "    print(context)\n",
    "    print('-'* 30 + \" LLM answer \" + '-'* 30)\n",
    "    print(ai_msg.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(model=\"llama3.1\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ Context of the Artifact ------------------------------\n",
      "The identifier of this artifact is \"ST2014\".\n",
      "This artifact was created from the following period: 1930-1959.\n",
      "This artifact was submitted on 2022-06-28.\n",
      "The description of this artifact is \"Jurk, daagse jurk of werkjurk.\"\n",
      "The publisher of this artifact is MOMU.\n",
      "The subject of this artifact includes jurk.\n",
      "The context of this artifact is \"Deze daagse huis- of werkjurk van bedrukt katoen is nog ongedragen. Het papieren etiket geeft inzicht in waar dit kledingstuk gekocht is: de Grand Magasins À Saint-Jacques te Reims.  Aan het begin van de jaren 1920 fuseren twee grote warenhuizen in Reims, La Samaritaine en À la Tour Saint-Jacques, onder de naam À Saint-Jacques. In deze nieuwe zaak in de Rue de Vesle verkoopt men voornamelijk dames- en kinderkleding. Tevens zijn er zijn verstelateliers, en klanten kunnen er terecht voor stoffen, interieurtextiel en een grote verscheidenheid aan accessoires. In een bijgebouw wordt heren-, jongens-, en werkkleding verkocht. De winkel wordt uiteindelijk in 1971 overgenomen door de Britse keten Marks & Spencer.  Ruimvallende jurken zoals dit exemplaar werden door vrouwen binnenshuis over de eigen kleding gedragen om deze te beschermen tijdens het (huishoudelijk) werk. De taille wordt bijeengehouden met een ceintuur, en de jurk heeft praktische ruime zakken. Door de knopen langs de gehele voorkant is de jurk snel uit te trekken indien er bezoek komt en men er deftig uit wil zien. Het is onzeker waarom deze jurk nooit gedragen is. Mogelijk komt de jurk uit een onverkochte winkelvoorraad van À Saint-Jacques.\".\n",
      "The preferred identifier of this artifact is http://data.momu.be/ark:34546/mjm68z.\n",
      "The current keeper of this artifact is Studiecollectie.\n",
      "The identifier of this artifact is ST2014.\n",
      "The group type of this artifact is  TMS.\n",
      "The group value of this artifact is 60190.\n",
      "The partNumber of this artifact is 74348.\n",
      "------------------------------ LLM answer ------------------------------\n",
      "这个艺术品是从1930-1959年间创建的。\n"
     ]
    }
   ],
   "source": [
    "item_id = '16157'\n",
    "query = \"这是个艺术品源于哪个时代创建\"\n",
    "chat_llm_multilingual(llm, query, item_id, context_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
